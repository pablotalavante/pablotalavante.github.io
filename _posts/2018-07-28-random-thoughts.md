---
layout: post
title: random thoughts
description:
summary: 
tags: []
---

The learning process of a neural network consists of adjusting a large number of parameters through training so that, given a certain input, it produces the desired output.

This could be thought of as the existence of an internal order within the system.
But for a random initialization of parameters, networks are capable of producing outputs in which an order does exist, which is not given by training, but by the mathematical structure on which the system is built.

It is the non-linearities and the stacking of layers of the network that provide a reflected order in the form of continuities where, apparently, there is disorder.

Despite the embryonic state of the system, where it has not yet evolved to become a "useful" system, there is an output that is not random, despite the fact that both the parameters and the choice of network architecture do they are.
What is shown are the outputs of different networks generated by this method.

![1](imgs/RT-1.png)

![2](imgs/RT-3.png)

![3](imgs/RT-5.png)

![4](imgs/RT-2.png)

![5](imgs/bRT-4.png)
